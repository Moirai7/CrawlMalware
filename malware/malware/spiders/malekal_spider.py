import scrapy
from scrapy.selector import Selector
import re
import subprocess

FILE_STORE='/Users/emma/Work/CrawlMalware/malware/malware/data/malware/'

class MalekalSpider(scrapy.Spider):
	name = "malekal"
	allowed_domains = ["malwaredb.malekal.com"]
	start_urls = ["http://malwaredb.malekal.com/"]
	#start_urls = ["file:///Users/emma/Work/CrawlMalware/malware/output.html"]
	
	def parse(self, response):
		sel = Selector(text=response.body,type="html")
		links = sel.xpath("//div[@align='center']/table/tr/td/a/@href").extract()
		for link in links:
			if link.find('files.php?file=') != -1:
				subprocess.Popen(["wget", response.urljoin(link), "-P", FILE_STORE])

		return
		if response.url.find('page') == -1:
			yield scrapy.Request(response.url+"/index.php?page=1", callback = self.parse)
		else:
			nextpage = int(re.findall('page=(\d{1,2})', response.url)[0])+1
			if nextpage < 50:
				yield scrapy.Request(url = re.sub('page=\d{1,2}','page='+str(nextpage), response.url), callback=self.parse)
	
