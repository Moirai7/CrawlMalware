import scrapy
from scrapy.selector import Selector
import re
import subprocess
from malware.items import FileDownloadItem


FILE_STORE='/Users/emma/Work/CrawlMalware/malware/malware/data/malware/'

class MalekalSpider(scrapy.Spider):
	name = "malekal"
	allowed_domains = ["malwaredb.malekal.com"]
	start_urls = ["http://malwaredb.malekal.com/index.php?page=86"]#50,51,52,85
	#start_urls = ["file:///Users/emma/Work/CrawlMalware/malware/output.html"]
	res = []
	
	def __init__(self):
		import os
		files= os.listdir(FILE_STORE)
		for fn in files:
			self.res.append(fn)
		print(len(self.res))
	
	def parse(self, response):
		sel = Selector(text=response.body,type="html")
		links = sel.xpath("//div[@align='center']/table/tr/td/a/@href").extract()
		for link in links:
			if link.lstrip('./') in self.res:
				continue
			if link.find('files.php?file=') != -1:
				self.res.append(link.lstrip('./'))
				link = response.urljoin(link)
				p = subprocess.Popen(["wget", link, "-P", FILE_STORE])
				p.wait()
		item = FileDownloadItem()
		#item['file_urls'] = self.res
		yield item
		if response.url.find('page') == -1:
			yield scrapy.Request(response.url+"/index.php?page=2", callback = self.parse)
		else:
			nextpage = int(re.findall('page=(\d{1,2})', response.url)[0])+1
			if nextpage < 829:
				yield scrapy.Request(url = re.sub('page=\d{1,2}','page='+str(nextpage), response.url), callback=self.parse)
	
