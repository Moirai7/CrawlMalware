# -*- coding: utf-8 -*-
import json
import scrapy
from urllib.parse import urljoin
import subprocess

'''
import asyncio
import logging
import os
import time
import redis
from threading import Thread
def redis_push(pool, name, url):
    r = redis.Redis(connection_pool=pool)
    r.lpush("apks", name + "," + url)


def get_redis():
    connection_pool = redis.ConnectionPool(host='127.0.0.1', port=6379, decode_responses=True)
    return redis.Redis(connection_pool=connection_pool)


def start_loop(loop):
    asyncio.set_event_loop(loop)
    loop.run_forever()


async def async_download(logger, name, url):
    logger.info('Start wget %s...' % name)
    # print('Start wget %s...' % name)
    name = name + ".apk"
    dir = "download_apks/" + name.split(".")[1][0]
    try:
        if not os.path.isdir(dir):
            os.mkdir(dir)
        if os.path.isfile(dir + "/" + name):
            logger.info("\n-----------------------------------------------\n"
                        "Already exists %s" % name +
                        "\n-----------------------------------------------\n")
            return
        command = 'wget "%s" -O %s/%s > /dev/null' % (url, dir, name)
        dl = await asyncio.create_subprocess_shell(command)
        await dl.wait()
        logger.info('Complete wget %s...' % name)
    except Exception as e:
        print(e)
        
def run():
    rcon = get_redis()
    new_loop = asyncio.new_event_loop()
    t = Thread(target=start_loop, args=(new_loop,))
    t.setDaemon(True)
    t.start()

    try:
        while True:
            value = rcon.rpop("apks")
            name, url = value.split(",")
            asyncio.run_coroutine_threadsafe(async_download(logger, name, url), new_loop)
    except Exception as e:
        print(e)
        new_loop.stop()
    finally:
        pass
'''


class ApkmonkSpider(scrapy.Spider):
    name = 'apkmonk'
    allowed_domains = ['apkmonk.com']
    start_urls = ['https://www.apkmonk.com/category/action/1/']
    base_url = 'https://www.apkmonk.com/category/action/{0}/'
    current_page = 1
    
    def parse(self, response):
        urls = response.xpath('//*[@class="row"]//a/@href').extract()
        urls = set(urls)
        urls = list(urls)
        for url in urls:
            url = urljoin(response.url, url)
            yield scrapy.Request(url, callback=self.parse_click)

        self.current_page += 1
        next_page = self.base_url.format(self.current_page)
        yield scrapy.Request(next_page, callback=self.parse)

    def parse_click(self, respone):
        click_url = respone.xpath('//*[@id="download_button"]/@href').extract_first()
        name, key = click_url.split("/")[-3:-1]
        json_url = urljoin(respone.url, "/down_file/?pkg={0}&key={1}".format(name, key))
        meta = {
            'name': name,
            'click_url': click_url,
        }
        yield scrapy.Request(json_url, meta=meta, callback=self.getDownloadJs)

    def getDownloadJs(self, response):
        re_json = json.loads(response.text)
        try:
            download_url = re_json["url"]
            p = subprocess.Popen(["wget", download_url, "-P", dir, "--limit-rate", '50k'])
            p.wait()            
        except Exception as e:
            print(e)
